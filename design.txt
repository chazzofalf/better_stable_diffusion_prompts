Create a Python project that uses Ollama to progressively build a Stable Diffusion prompt.

The program accepts input in two ways:

1. **Interactive mode** – Input is received line by line until a line containing exactly **THE END** is encountered, then the program ends.
2. **File mode** – Command‑line arguments are treated as filenames. The files are read in the order provided, their contents are concatenated (newline‑separated), and the combined text is sent to Ollama in a single prompt (no **THE END** required).

For each input (whether interactive or from files), Ollama is given a prompt that builds a mental image and returns the following items:

* Positive Stable Diffusion prompt  
* Negative Stable Diffusion prompt  
* CFG Scale  
* Optimum Image Resolution  
* Steps (up to 500)  
* Scheduler (one of the listed options)

**Additional instruction** – The prompt now includes the phrase **“Use proper weights for drawn elements.”** to guide Ollama’s weighting of visual components.

**Model & CLI details**

* The prompt presented to Ollama references **model Juggernaut XL v9, no LoRA**.  
* The actual CLI call uses the `gemma3:27b` model (`ollama run gemma3:27b …`). This mismatch is intentional: the script keeps the placeholder CLI call while the prompt tells Ollama to assume the desired model.

**Output handling**

* An optional `-o <filename>` flag can be supplied; if present, the generated Stable Diffusion parameters are appended to the specified file in addition to being printed to stdout.
* An optional `-m <model>` flag allows specifying an alternative Ollama model name (default `gemma3:27b`). Example usage:

  ```bash
  ./main.py -m llama2:13b description.txt
  ```

**Scheduler list (as per design)**

- DDIM
- DDPM
- DEIS
- DEIS Karras
- DPM++ 2S
- DPM++ 2S Karras
- DPM++ 2M
- DPM++ 2M Karras
- DPM++ 2M SDE
- DPM++ 2M SDE Karras
- DPM 3M
- DPM 3M Karras
- DPM 3M SDE
- DPM 3M Karras
- Euler
- Euler Karras
- Eular Ancestral
- Heun
- Heun Karras
- KDPM 2
- KDPM 2 Karras
- KDPM 2 Ancestral
- KDPM 2 Ancestral Karras
- LCM
- LMS
- LMS Karras
- PNDM
- TCD
- UniPC
- UniPC Karras

**Project state (as of 2025‑08‑24)**

* `main.py` implements both interactive and file modes, includes the `-o` output flag, and uses the placeholder CLI call described above.  
* The script falls back to deterministic placeholder output if the Ollama CLI invocation fails.  
* No code writes back to `cline.md`; all documentation is now stored explicitly in `design.txt` and `cline.md`.  
* The repository contains an example prompt description (`example1.txt`) and a comprehensive context file (`cline.md`) that aggregates all project artifacts.
